{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naman9810/Fake-News-Detection/blob/main/SCRAPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGQ6czHsUOXm"
      },
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z_yGhs0UTEN"
      },
      "source": [
        "def scrape(words, date_since, numtweet):\n",
        "  tweets = tweepy.Cursor(api.search, q=words, lang=\"en\",\n",
        "\t\t\t\t\t\tsince=date_since, tweet_mode='extended').items(numtweet)\n",
        "  list_tweets = [tweet for tweet in tweets]\n",
        "  tweet_texts=[]\n",
        "  for tweet in list_tweets:\n",
        "    retweetcount = tweet.retweet_count\n",
        "    hashtags = tweet.entities['hashtags']\n",
        "    try:\n",
        "      text = tweet.retweeted_status.full_text\n",
        "    except AttributeError:\n",
        "      text = tweet.full_text\n",
        "    print(f\"Tweet Text:{text}\")\n",
        "    tweet_texts.append(text)\n",
        "  x={'tweets':tweet_texts}\n",
        "  e_t=pd.DataFrame(x)\n",
        "  return e_t,tweet_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSESgZbjBeJ"
      },
      "source": [
        "consumer_key = \"mid9bG90uqcpmhsZqhn1BM7QX\"\n",
        "consumer_secret = \"90SqzgW1J6QTmBM0Vnx5GSWUHhRRTGyovX26Mk6qxuxpt81wYU\"\n",
        "access_key = \"1291629689556131840-wYf8Say4YUYhoEu3ujmNWltKQjfcHs\"\n",
        "access_secret = \"aM3EgNQzCSnNL2dCIOoY8HSYQXlZUHvimYEndltkfwQv1\"\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6OcFCzIjFbI"
      },
      "source": [
        "print(\"Enter Twitter HashTag:\")\n",
        "words = input()\n",
        "print('Enter Date since The Tweets are required(YYYY-MM-DD):')\n",
        "date_since = input()\n",
        "print('Enter the number of tweets to be scrapped:')\n",
        "numtweet=int(input())\n",
        "print('EXTRACTED TWEETS:-')\n",
        "extracted_tweets,tweets=scrape(words, date_since, numtweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR3AI4lAaC3W"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(extracted_tweets['tweets'])\n",
        "extracted_tweets['tweets']= tokenizer.texts_to_sequences(extracted_tweets['tweets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ6XnzaaBgmJ"
      },
      "source": [
        "len_text = [len(x) for x in extracted_tweets['tweets'].values]\n",
        "len_text = np.array(len_text)\n",
        "m=np.mean(len_text)\n",
        "std=np.std(len_text)\n",
        "MAX_TEXT=int(2*std+m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyZtl708BQir"
      },
      "source": [
        "X= tf.keras.preprocessing.sequence.pad_sequences(extracted_tweets['tweets'],maxlen=MAX_TEXT,padding='post',\n",
        "                                                            truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRd_0bRez2fG"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYJHAYC9ydEd"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/gdrive/MyDrive/fake_news/saved_model/my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuXFaoXyij_"
      },
      "source": [
        "predict=model.predict(X)\n",
        "for i in range(len(predict)):\n",
        "  if predict[i]<0.5:\n",
        "    print(tweets[i],' : FAKE')\n",
        "  else:\n",
        "    print(tweets[i],\" : REAL\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODUK5ZXi18nF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}