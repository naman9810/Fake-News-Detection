{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naman9810/Fake-News-Detection/blob/main/major_project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcVh0u38eoLP"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import regularizers\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRuOANKve6dj"
      },
      "source": [
        "def parse(filename):\n",
        "  items = []\n",
        "  with open(filename, 'r') as f:\n",
        "    data = f.read()[1:-2]\n",
        "    data = data.split('}\\n{')\n",
        "    for item in data:\n",
        "      item_dict = json.loads('{' + item + '}')\n",
        "      items.append(item_dict)\n",
        "  return items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAoWU4ZYe_zI"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YPJ3brBfDCh"
      },
      "source": [
        "os.chdir(\"/content/gdrive/MyDrive/fake_news\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwjSZr7PfOGW"
      },
      "source": [
        "data_list=parse('train.jsonl')\n",
        "data=pd.DataFrame(data_list)\n",
        "data.to_csv('train.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUbPwaSbfQ29"
      },
      "source": [
        "data['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVj9ns8HfT5V"
      },
      "source": [
        "data=data[data.label!='NOT ENOUGH INFO']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiw7GXm0fXd7"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK9THU1EfZ09"
      },
      "source": [
        "data.drop('id',axis=1,inplace=True)\n",
        "data.drop('verifiable',axis=1,inplace=True)\n",
        "data.drop('evidence',axis=1,inplace=True)\n",
        "d=pd.get_dummies(data['label'],drop_first=True)\n",
        "data.drop('label',axis=1,inplace=True)\n",
        "data=pd.concat([data,d],axis=1)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niWhDNzwfcc-"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(data['claim'])\n",
        "data['claim'] = tokenizer.texts_to_sequences(data['claim'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnPHdePIfgIP"
      },
      "source": [
        "len_text = [len(x) for x in data['claim'].values]\n",
        "len_text = np.array(len_text)\n",
        "sns.distplot(len_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBoWLibzfiB7"
      },
      "source": [
        "m=np.mean(len_text)\n",
        "std=np.std(len_text)\n",
        "MAX_TEXT=int(2*std+m)\n",
        "print(MAX_TEXT)\n",
        "np.unique((len_text >= MAX_TEXT), return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sTr_rWMfkkm"
      },
      "source": [
        "labels = data.pop('SUPPORTS')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data,labels,test_size=0.05,random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKVoFnREfm9y"
      },
      "source": [
        "X_train_claim = tf.keras.preprocessing.sequence.pad_sequences(X_train['claim'],maxlen=MAX_TEXT,padding='post',\n",
        "                                                             truncating='post')\n",
        "X_test_claim = tf.keras.preprocessing.sequence.pad_sequences(X_test['claim'],maxlen=MAX_TEXT,padding='post',\n",
        "                                                            truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BvK6lYafpXq"
      },
      "source": [
        "embeddings_matrix = np.zeros((len(tokenizer.index_word)+1, 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBw_CYIZfrpo"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(tokenizer.index_word)+1, 100,activity_regularizer=regularizers.l2(2e-3),trainable=False),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu',activity_regularizer=regularizers.l2(2e-3)),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "    tf.keras.layers.LSTM(20, return_sequences=True,activity_regularizer=regularizers.l2(2e-3)),\n",
        "    tf.keras.layers.LSTM(20,activity_regularizer=regularizers.l2(2e-3)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1024,activation='relu',activity_regularizer=regularizers.l2(2e-3)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512,activation='relu',activity_regularizer=regularizers.l2(2e-3)),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\",activity_regularizer=regularizers.l2(2e-3))\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gK3btxJf2cG"
      },
      "source": [
        "history = model.fit(X_train_claim,Y_train, epochs=100, batch_size=256, validation_data=(X_test_claim,Y_test))\n",
        "model.save('saved_model/my_model',include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4eVp5VXDvjX"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/gdrive/MyDrive/fake_news/saved_model/my_model')\n",
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "acc_train=model.evaluate(X_train_claim,Y_train)\n",
        "acc_test=model.evaluate(X_test_claim,Y_test)\n",
        "print('Training Accuracy:',acc_train[1])\n",
        "print('Test Accuracy:',acc_test[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtyObVZWgsBn"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIWkb7ZAxBRb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}